{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e607dbf",
   "metadata": {},
   "source": [
    "# Benchmarking Using HellaSwag Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d0b7ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind: question index\n",
    "# id: question id\n",
    "# activity_label: A short phrase describing the events in the question\n",
    "# ctx: The full context for the question\n",
    "# ctx_a: The first sentence of the context\n",
    "# ctx_b: The second sentence of the context\n",
    "# dataset: Domain of the question -- e.g. activitynet / wikihow\n",
    "# ending_options: A list of four ending choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9395b236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2023-08-14 18:17:54.370561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a different task than the one specified in the repository. Be sure to know what you're doing :)\n",
      "WARNING:huggingface_hub.inference_api:You're using a different task than the one specified in the repository. Be sure to know what you're doing :)\n",
      "/home/hashem/Desktop/workspace/prompttools/prompttools/benchmarks/benchmark.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.benchmark_df[\"response_options\"] = self.response_options\n",
      "llama.cpp: loading model from ../llama/llama.cpp/models/7b/ggml-vicuna-7b-1.1-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 3917.73 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "\n",
      "llama_print_timings:        load time = 10515.25 ms\n",
      "llama_print_timings:      sample time =   401.33 ms /    74 runs   (    5.42 ms per token,   184.39 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10515.19 ms /    27 tokens (  389.45 ms per token,     2.57 tokens per second)\n",
      "llama_print_timings:        eval time = 39695.29 ms /    73 runs   (  543.77 ms per token,     1.84 tokens per second)\n",
      "llama_print_timings:       total time = 51640.48 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Llama.generate: prefix-match hit\n",
      "llama_print_timings:        load time = 10515.25 ms\n",
      "llama_print_timings:      sample time =   378.99 ms /    70 runs   (    5.41 ms per token,   184.70 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11860.23 ms /    25 tokens (  474.41 ms per token,     2.11 tokens per second)\n",
      "llama_print_timings:        eval time = 41769.87 ms /    69 runs   (  605.36 ms per token,     1.65 tokens per second)\n",
      "llama_print_timings:       total time = 54956.14 ms\n",
      "\n",
      "llama_print_timings:        load time = 10515.25 ms\n",
      "llama_print_timings:      sample time =   678.63 ms /   128 runs   (    5.30 ms per token,   188.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8860.19 ms /    18 tokens (  492.23 ms per token,     2.03 tokens per second)\n",
      "llama_print_timings:        eval time = 66411.33 ms /   127 runs   (  522.92 ms per token,     1.91 tokens per second)\n",
      "llama_print_timings:       total time = 77640.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Llama.generate: prefix-match hit\n",
      "llama_print_timings:        load time = 10515.25 ms\n",
      "llama_print_timings:      sample time =   469.87 ms /   128 runs   (    3.67 ms per token,   272.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9813.84 ms /    37 tokens (  265.24 ms per token,     3.77 tokens per second)\n",
      "llama_print_timings:        eval time = 31885.91 ms /   127 runs   (  251.07 ms per token,     3.98 tokens per second)\n",
      "llama_print_timings:       total time = 43324.75 ms\n",
      "/home/hashem/Desktop/workspace/prompttools/prompttools/benchmarks/benchmark.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.benchmark_df[\"response_options\"] = self.response_options\n",
      "\n",
      "llama_print_timings:        load time = 10515.25 ms\n",
      "llama_print_timings:      sample time =    94.59 ms /    26 runs   (    3.64 ms per token,   274.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6730.98 ms /    29 tokens (  232.10 ms per token,     4.31 tokens per second)\n",
      "llama_print_timings:        eval time =  6239.03 ms /    25 runs   (  249.56 ms per token,     4.01 tokens per second)\n",
      "llama_print_timings:       total time = 13288.64 ms\n",
      "llama.cpp: loading model from ../llama/llama.cpp/models/13b/ggml-vic13b-uncensored-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 7349.72 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "\n",
      "llama_print_timings:        load time = 23500.53 ms\n",
      "llama_print_timings:      sample time =   246.46 ms /    44 runs   (    5.60 ms per token,   178.53 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23500.47 ms /    27 tokens (  870.39 ms per token,     1.15 tokens per second)\n",
      "Llama.generate: prefix-match hit\n",
      "llama_print_timings:        eval time = 45151.57 ms /    43 runs   ( 1050.04 ms per token,     0.95 tokens per second)\n",
      "llama_print_timings:       total time = 69533.09 ms\n",
      "\n",
      "llama_print_timings:        load time = 23500.53 ms\n",
      "llama_print_timings:      sample time =   422.45 ms /    76 runs   (    5.56 ms per token,   179.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23110.95 ms /    25 tokens (  924.44 ms per token,     1.08 tokens per second)\n",
      "llama_print_timings:        eval time = 79182.27 ms /    75 runs   ( 1055.76 ms per token,     0.95 tokens per second)\n",
      "llama_print_timings:       total time = 103803.91 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 23500.53 ms\n",
      "llama_print_timings:      sample time =   104.40 ms /    18 runs   (    5.80 ms per token,   172.42 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16502.59 ms /    18 tokens (  916.81 ms per token,     1.09 tokens per second)\n",
      "Llama.generate: prefix-match hit\n",
      "llama_print_timings:        eval time = 17309.88 ms /    17 runs   ( 1018.23 ms per token,     0.98 tokens per second)\n",
      "llama_print_timings:       total time = 34178.23 ms\n",
      "\n",
      "llama_print_timings:        load time = 23500.53 ms\n",
      "llama_print_timings:      sample time =   325.36 ms /    59 runs   (    5.51 ms per token,   181.34 tokens per second)\n",
      "llama_print_timings: prompt eval time = 32580.36 ms /    37 tokens (  880.55 ms per token,     1.14 tokens per second)\n",
      "llama_print_timings:        eval time = 61438.21 ms /    58 runs   ( 1059.28 ms per token,     0.94 tokens per second)\n",
      "llama_print_timings:       total time = 95182.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 23500.53 ms\n",
      "llama_print_timings:      sample time =   641.12 ms /   113 runs   (    5.67 ms per token,   176.25 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26815.12 ms /    29 tokens (  924.66 ms per token,     1.08 tokens per second)\n",
      "llama_print_timings:        eval time = 118333.10 ms /   112 runs   ( 1056.55 ms per token,     0.95 tokens per second)\n",
      "llama_print_timings:       total time = 147449.79 ms\n",
      "/home/hashem/Desktop/workspace/prompttools/prompttools/benchmarks/benchmark.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.benchmark_df[\"response_options\"] = self.response_options\n",
      "/tmp/ipykernel_160625/2438045213.py:105: UserWarning: Column 'prompt' does not exist. Using column 'messages' instead.\n",
      "  openai_chat_results = benchmarking_openai_chat.multiple_choice_benchmark()\n",
      "/home/hashem/Desktop/workspace/prompttools/prompttools/benchmarks/benchmark.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.benchmark_df[\"response_options\"] = self.response_options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0:12:26.063200\n"
     ]
    }
   ],
   "source": [
    "from prompttools.benchmarks import Benchmark\n",
    "from prompttools.experiment import (\n",
    "    LlamaCppExperiment,\n",
    "    OpenAIChatExperiment,\n",
    "    HuggingFaceHubExperiment,\n",
    ")\n",
    "from prompttools.utils import semantic_similarity\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "data = []\n",
    "with open('prompttools/data/benchmarking/hellaswag/train.jsonl', 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            json_obj = json.loads(line)\n",
    "            data.append([json_obj['ctx'], json_obj['ending_options']])\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Skipped invalid JSON: {line}\")\n",
    "labels = []\n",
    "with open('prompttools/data/benchmarking/hellaswag/train-labels.lst', 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            json_obj = json.loads(line)\n",
    "            labels.append(json_obj)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Skipped invalid JSON: {line}\")\n",
    "\n",
    "hella_swag = pd.DataFrame(data, columns=['ctx', 'ending_options'])\n",
    "hella_swag[\"labels\"] = labels\n",
    "hella_swag = hella_swag.head(5)\n",
    "\n",
    "sample_ctxs = hella_swag['ctx'].values\n",
    "sample_ending_options = hella_swag['ending_options'].values\n",
    "sample_labels = hella_swag['labels'].values\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "print(f\"Start time: {start}\")\n",
    "\n",
    "temperatures = [0.5]\n",
    "\n",
    "models = [\"google/flan-t5-xxl\"]\n",
    "prompts = sample_ctxs\n",
    "task = [\"text-generation\"]\n",
    "google_flan_t5_xxl_experiment = HuggingFaceHubExperiment(\n",
    "    models, prompts, task, temperature=temperatures\n",
    ")\n",
    "benchmarking_google_flan_t5_xxl = Benchmark(\n",
    "    experiment=google_flan_t5_xxl_experiment,\n",
    "    eval_method=semantic_similarity,\n",
    "    prompts=sample_ctxs,\n",
    "    response_options=sample_ending_options,\n",
    "    correct_response_indices=sample_labels\n",
    ")\n",
    "google_flan_t5_xxl_results = benchmarking_google_flan_t5_xxl.multiple_choice_benchmark()\n",
    "\n",
    "vicuna7b_experiment = LlamaCppExperiment(\n",
    "    [\n",
    "        \"../llama/llama.cpp/models/7b/ggml-vicuna-7b-1.1-q4_0.bin\",\n",
    "    ],\n",
    "    sample_ctxs,\n",
    "    call_params=dict(temperature=temperatures),\n",
    ")\n",
    "benchmarking_vicuna7b = Benchmark(\n",
    "    experiment=vicuna7b_experiment,\n",
    "    eval_method=semantic_similarity,\n",
    "    prompts=sample_ctxs,\n",
    "    response_options=sample_ending_options,\n",
    "    correct_response_indices=sample_labels\n",
    ")\n",
    "vicuna7b_results = benchmarking_vicuna7b.multiple_choice_benchmark()\n",
    "\n",
    "vicuna13b_experiment = LlamaCppExperiment(\n",
    "    [\n",
    "        \"../llama/llama.cpp/models/13b/ggml-vic13b-uncensored-q4_0.bin\",\n",
    "    ],\n",
    "    sample_ctxs,\n",
    "    call_params=dict(temperature=temperatures),\n",
    ")\n",
    "benchmarking_vicuna13b = Benchmark(\n",
    "    experiment=vicuna13b_experiment,\n",
    "    eval_method=semantic_similarity,\n",
    "    prompts=sample_ctxs,\n",
    "    response_options=sample_ending_options,\n",
    "    correct_response_indices=sample_labels\n",
    ")\n",
    "vicuna13b_results = benchmarking_vicuna13b.multiple_choice_benchmark()\n",
    "\n",
    "openai_chat_experiment = OpenAIChatExperiment(\n",
    "    [\"gpt-3.5-turbo\"],\n",
    "    [\n",
    "        [{\"role\": \"system\", \"content\": c}]\n",
    "        for c in sample_ctxs\n",
    "    ],\n",
    "    temperature=temperatures\n",
    ")\n",
    "benchmarking_openai_chat = Benchmark(\n",
    "    experiment=openai_chat_experiment,\n",
    "    eval_method=semantic_similarity,\n",
    "    prompts=sample_ctxs,\n",
    "    response_options=sample_ending_options,\n",
    "    correct_response_indices=sample_labels,\n",
    ")\n",
    "openai_chat_results = benchmarking_openai_chat.multiple_choice_benchmark()\n",
    "\n",
    "print(\"Time taken: \", datetime.datetime.now() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c63e5335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   google_flan_t5_xxl  vicuna7b  vicuna13b  openai_chat\n",
      "0  0.2                 0.0       0.2        0.4        \n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(\n",
    "    data=[[google_flan_t5_xxl_results, vicuna7b_results, vicuna13b_results, openai_chat_results]],\n",
    "    columns=[\"google_flan_t5_xxl\", \"vicuna7b\", \"vicuna13b\", \"openai_chat\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "eec05f12730ef3ef66f433616fcd3cfdacd3dcf1f1c49c706eaa0465be8f325b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
