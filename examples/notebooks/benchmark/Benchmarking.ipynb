{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e607dbf",
   "metadata": {},
   "source": [
    "# Benchmarking Using HellaSwag Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d0b7ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind: question index\n",
    "# id: question id\n",
    "# activity_label: A short phrase describing the events in the question\n",
    "# ctx: The full context for the question\n",
    "# ctx_a: The first sentence of the context\n",
    "# ctx_b: The second sentence of the context\n",
    "# dataset: Domain of the question -- e.g. activitynet / wikihow\n",
    "# ending_options: A list of four ending choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9395b236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2023-08-14 22:02:16.655766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a different task than the one specified in the repository. Be sure to know what you're doing :)\n",
      "WARNING:huggingface_hub.inference_api:You're using a different task than the one specified in the repository. Be sure to know what you're doing :)\n",
      "/home/hashem/Desktop/workspace/prompttools/prompttools/benchmarks/benchmark.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  benchmark_df[\"response_options\"] = self.response_options\n",
      "llama.cpp: loading model from ../llama/llama.cpp/models/7b/ggml-vicuna-7b-1.1-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 5407.72 MB (+ 1026.00 MB per state)\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "llama_print_timings:        load time =  1705.06 ms\n",
      "llama_print_timings:      sample time =    72.14 ms /    74 runs   (    0.97 ms per token,  1025.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1705.03 ms /    27 tokens (   63.15 ms per token,    15.84 tokens per second)\n",
      "llama_print_timings:        eval time =  7709.41 ms /    73 runs   (  105.61 ms per token,     9.47 tokens per second)\n",
      "llama_print_timings:       total time =  9679.11 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  1705.06 ms\n",
      "llama_print_timings:      sample time =    78.94 ms /    70 runs   (    1.13 ms per token,   886.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1770.74 ms /    25 tokens (   70.83 ms per token,    14.12 tokens per second)\n",
      "llama_print_timings:        eval time =  7176.92 ms /    69 runs   (  104.01 ms per token,     9.61 tokens per second)\n",
      "llama_print_timings:       total time =  9225.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  1705.06 ms\n",
      "llama_print_timings:      sample time =   148.22 ms /   128 runs   (    1.16 ms per token,   863.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1268.79 ms /    18 tokens (   70.49 ms per token,    14.19 tokens per second)\n",
      "llama_print_timings:        eval time = 13569.95 ms /   127 runs   (  106.85 ms per token,     9.36 tokens per second)\n",
      "llama_print_timings:       total time = 15356.95 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  1705.06 ms\n",
      "llama_print_timings:      sample time =   150.77 ms /   128 runs   (    1.18 ms per token,   848.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2509.09 ms /    37 tokens (   67.81 ms per token,    14.75 tokens per second)\n",
      "llama_print_timings:        eval time = 13462.61 ms /   127 runs   (  106.00 ms per token,     9.43 tokens per second)\n",
      "llama_print_timings:       total time = 16496.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  1705.06 ms\n",
      "llama_print_timings:      sample time =    29.64 ms /    26 runs   (    1.14 ms per token,   877.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1979.85 ms /    29 tokens (   68.27 ms per token,    14.65 tokens per second)\n",
      "llama_print_timings:        eval time =  2623.61 ms /    25 runs   (  104.94 ms per token,     9.53 tokens per second)\n",
      "llama_print_timings:       total time =  4705.84 ms\n",
      "/home/hashem/Desktop/workspace/prompttools/prompttools/benchmarks/benchmark.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  benchmark_df[\"response_options\"] = self.response_options\n",
      "llama.cpp: loading model from ../llama/llama.cpp/models/13b/ggml-vic13b-uncensored-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 9031.71 MB (+ 1608.00 MB per state)\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "llama_print_timings:        load time =  3959.51 ms\n",
      "llama_print_timings:      sample time =    50.13 ms /    44 runs   (    1.14 ms per token,   877.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3959.46 ms /    27 tokens (  146.65 ms per token,     6.82 tokens per second)\n",
      "llama_print_timings:        eval time =  8407.01 ms /    43 runs   (  195.51 ms per token,     5.11 tokens per second)\n",
      "llama_print_timings:       total time = 12545.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3959.51 ms\n",
      "llama_print_timings:      sample time =    88.07 ms /    76 runs   (    1.16 ms per token,   862.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3281.42 ms /    25 tokens (  131.26 ms per token,     7.62 tokens per second)\n",
      "llama_print_timings:        eval time = 14461.05 ms /    75 runs   (  192.81 ms per token,     5.19 tokens per second)\n",
      "llama_print_timings:       total time = 18054.98 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3959.51 ms\n",
      "llama_print_timings:      sample time =    22.49 ms /    18 runs   (    1.25 ms per token,   800.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2360.65 ms /    18 tokens (  131.15 ms per token,     7.63 tokens per second)\n",
      "llama_print_timings:        eval time =  3253.09 ms /    17 runs   (  191.36 ms per token,     5.23 tokens per second)\n",
      "llama_print_timings:       total time =  5688.18 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "from prompttools.benchmarks import Benchmark\n",
    "from prompttools.experiment import (\n",
    "    LlamaCppExperiment,\n",
    "    OpenAIChatExperiment,\n",
    "    HuggingFaceHubExperiment,\n",
    ")\n",
    "from prompttools.utils import semantic_similarity\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "data = []\n",
    "with open('prompttools/data/benchmarking/hellaswag/hellaswag_dataset.jsonl', 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            json_obj = json.loads(line)\n",
    "            data.append([json_obj['ctx'], json_obj['ending_options']])\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Skipped invalid JSON: {line}\")\n",
    "labels = []\n",
    "with open('prompttools/data/benchmarking/hellaswag/hellaswag_labels.lst', 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            json_obj = json.loads(line)\n",
    "            labels.append(json_obj)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Skipped invalid JSON: {line}\")\n",
    "\n",
    "hella_swag = pd.DataFrame(data, columns=['ctx', 'ending_options'])\n",
    "hella_swag[\"labels\"] = labels\n",
    "hella_swag = hella_swag.head(5)\n",
    "\n",
    "sample_ctxs = hella_swag['ctx'].values\n",
    "sample_ending_options = hella_swag['ending_options'].values\n",
    "sample_labels = hella_swag['labels'].values\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "print(f\"Start time: {start}\")\n",
    "\n",
    "temperatures = [0.5]\n",
    "\n",
    "models = [\"google/flan-t5-xxl\"]\n",
    "prompts = sample_ctxs\n",
    "task = [\"text-generation\"]\n",
    "google_flan_t5_xxl_experiment = HuggingFaceHubExperiment(\n",
    "    models, prompts, task, temperature=temperatures\n",
    ")\n",
    "benchmarking_google_flan_t5_xxl = Benchmark(\n",
    "    experiment=google_flan_t5_xxl_experiment,\n",
    "    eval_method=semantic_similarity,\n",
    "    prompts=sample_ctxs,\n",
    "    response_options=sample_ending_options,\n",
    "    correct_response_indices=sample_labels\n",
    ")\n",
    "google_flan_t5_xxl_results = benchmarking_google_flan_t5_xxl.multiple_choice_benchmark()\n",
    "\n",
    "vicuna7b_experiment = LlamaCppExperiment(\n",
    "    [\n",
    "        \"../llama/llama.cpp/models/7b/ggml-vicuna-7b-1.1-q4_0.bin\",\n",
    "    ],\n",
    "    sample_ctxs,\n",
    "    call_params=dict(temperature=temperatures),\n",
    ")\n",
    "benchmarking_vicuna7b = Benchmark(\n",
    "    experiment=vicuna7b_experiment,\n",
    "    eval_method=semantic_similarity,\n",
    "    prompts=sample_ctxs,\n",
    "    response_options=sample_ending_options,\n",
    "    correct_response_indices=sample_labels\n",
    ")\n",
    "vicuna7b_results = benchmarking_vicuna7b.multiple_choice_benchmark()\n",
    "\n",
    "vicuna13b_experiment = LlamaCppExperiment(\n",
    "    [\n",
    "        \"../llama/llama.cpp/models/13b/ggml-vic13b-uncensored-q4_0.bin\",\n",
    "    ],\n",
    "    sample_ctxs,\n",
    "    call_params=dict(temperature=temperatures),\n",
    ")\n",
    "benchmarking_vicuna13b = Benchmark(\n",
    "    experiment=vicuna13b_experiment,\n",
    "    eval_method=semantic_similarity,\n",
    "    prompts=sample_ctxs,\n",
    "    response_options=sample_ending_options,\n",
    "    correct_response_indices=sample_labels\n",
    ")\n",
    "vicuna13b_results = benchmarking_vicuna13b.multiple_choice_benchmark()\n",
    "\n",
    "openai_chat_experiment = OpenAIChatExperiment(\n",
    "    [\"gpt-3.5-turbo\"],\n",
    "    [\n",
    "        [{\"role\": \"system\", \"content\": c}]\n",
    "        for c in sample_ctxs\n",
    "    ],\n",
    "    temperature=temperatures\n",
    ")\n",
    "benchmarking_openai_chat = Benchmark(\n",
    "    experiment=openai_chat_experiment,\n",
    "    eval_method=semantic_similarity,\n",
    "    prompts=sample_ctxs,\n",
    "    response_options=sample_ending_options,\n",
    "    correct_response_indices=sample_labels,\n",
    ")\n",
    "openai_chat_results = benchmarking_openai_chat.multiple_choice_benchmark()\n",
    "\n",
    "print(\"Time taken: \", datetime.datetime.now() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c63e5335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   google_flan_t5_xxl  vicuna7b  vicuna13b  openai_chat\n",
      "0  0.2                 0.0       0.2        0.4        \n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(\n",
    "    data=[[google_flan_t5_xxl_results, vicuna7b_results, vicuna13b_results, openai_chat_results]],\n",
    "    columns=[\"google_flan_t5_xxl\", \"vicuna7b\", \"vicuna13b\", \"openai_chat\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "eec05f12730ef3ef66f433616fcd3cfdacd3dcf1f1c49c706eaa0465be8f325b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
